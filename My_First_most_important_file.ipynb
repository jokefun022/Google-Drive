{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/Google-Drive/blob/main/My_First_most_important_file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEWVmN7LpQvp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, make_scorer\n",
        "from time import time\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import joblib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHjv9Cu-jLeE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # To read dataset in dataframe style\n",
        "import numpy as np # To perform mathematical computations\n",
        "import tensorflow as tf # To Implement Neural Networks\n",
        "import re # Regular Expression\n",
        "import matplotlib.pyplot as plt # For Visualization\n",
        "import seaborn as sns # For visualization\n",
        "from bs4 import BeautifulSoup # To remove html tags\n",
        "import unicodedata # To remove accented characters\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fPaxXYKMF2F"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)  # Change -1 to None or a nonnegative integer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiqucZ0_sxHV"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Flatten,Embedding,Dropout,LSTM\n",
        "from tensorflow.keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F8UOk8-qZNB",
        "outputId": "97a3ecfb-cc63-433c-c1ad-327c44d22714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas sklearn torch transformers gensim fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "djSMG57PxG2T",
        "outputId": "2600f8b0-a97c-4dd8-f53f-0ea3363101ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim",
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "59ad6c26186940f29c21d8adb641b489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Collecting numpy\n",
            "  Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Using cached numpy-2.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-2.2.5\n",
            "Collecting gensim\n",
            "  Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "Collecting smart-open>=1.8.1 (from gensim)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open>=1.8.1->gensim)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Using cached gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Using cached scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Installing collected packages: wrapt, numpy, smart-open, scipy, gensim\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.5\n",
            "    Uninstalling numpy-2.2.5:\n",
            "      Successfully uninstalled numpy-2.2.5\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.1.0 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim",
                  "smart_open",
                  "wrapt"
                ]
              },
              "id": "77045d54b7d543489366f060e6b5ba07"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall gensim\n",
        "\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade --force-reinstall gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "# ... (rest of your imports) ...\n",
        "\n",
        "# Load dataset\n",
        "# Make sure the file path is correct. If the file is in a different directory:\n",
        "# train_file_path = \"/path/to/your/file/Data_With_Profiles.csv\"\n",
        "# test_file_path = \"/path/to/your/file/Data_With_Profiles.csv\"\n",
        "\n",
        "# If you uploaded the file or mounted Google Drive:\n",
        "# train_file_path = \"/content/Data_With_Profiles.csv\"  # If uploaded\n",
        "# test_file_path = \"/content/Data_With_Profiles.csv\"  # If uploaded\n",
        "# or\n",
        "# train_file_path = \"/content/drive/MyDrive/path/to/your/file/Data_With_Profiles.csv\"  # If in Drive\n",
        "# test_file_path = \"/content/drive/MyDrive/path/to/your/file/Data_With_Profiles.csv\"  # If in Drive\n",
        "\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# ... (rest of your code) ..."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "j31-Bk57SgTo",
        "outputId": "f1bc4876-fa56-4bb3-ecca-779257f68ba1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data_With_Profiles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-258d39db7032>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# test_file_path = \"/content/drive/MyDrive/path/to/your/file/Data_With_Profiles.csv\"  # If in Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_With_Profiles.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_-sOv54qfO4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "collapsed": true,
        "outputId": "d87f9df4-0224-4e46-a6a8-a734b7baa363"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Data_With_Profiles.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-46ad8d3d8549>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# test_file_path = \"../Dataset/Data_With_Profiles.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data_With_Profiles.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from gensim.models import Word2Vec, FastText # gensim is used here\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#import fasttext.util # This line was causing the error, This import is not necessary for the provided code\n",
        "\n",
        "# Load dataset\n",
        "# Check and correct the file path\n",
        "# If the dataset is in the same directory as the notebook, use:\n",
        "train_file_path = \"Data_With_Profiles.csv\"  # Make sure this path is correct\n",
        "test_file_path = \"Data_With_Profiles.csv\"  # Make sure this path is correct\n",
        "\n",
        "# If the dataset is in a different directory, adjust the path accordingly.\n",
        "# For example, if it's one directory above:\n",
        "# train_file_path = \"../Dataset/Data_With_Profiles.csv\"\n",
        "# test_file_path = \"../Dataset/Data_With_Profiles.csv\"\n",
        "\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# ... rest of your code ...\n",
        "\n",
        "# Rename columns (Modify according to your dataset)\n",
        "# Assuming your text column is named 'pichwara se akhrot kahe fod rahe ho chicha' and label column is named '1'\n",
        "df_train.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "df_test.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train[\"text\"], df_train[\"label\"], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Get the current working directory\n",
        "current_directory = os.getcwd()\n",
        "print(f\"Current working directory: {current_directory}\")\n",
        "\n",
        "# List files in the current directory\n",
        "print(\"Files in current directory:\")\n",
        "for file in os.listdir(current_directory):\n",
        "    print(file)\n",
        "\n",
        "# Check if the file exists\n",
        "file_name = \"Data_With_Profiles.csv\"\n",
        "if os.path.exists(file_name):\n",
        "    print(f\"File '{file_name}' found!\")\n",
        "    df_train = pd.read_csv(file_name)\n",
        "    df_test = pd.read_csv(file_name)\n",
        "else:\n",
        "    print(f\"File '{file_name}' not found. Please check the file path.\")\n",
        "    # If the file is in a different directory, specify the full path\n",
        "    # For example:\n",
        "    # file_path = \"/path/to/your/file/Data_With_Profiles.csv\"\n",
        "    # df_train = pd.read_csv(file_path)\n",
        "    # df_test = pd.read_csv(file_path)\n",
        "\n",
        "# ... rest of your code ..."
      ],
      "metadata": {
        "id": "sUHA_TOURL3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-eWZXJ4qvWCV"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade --force-reinstall gensim\n",
        "!pip install --upgrade scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9X8AypWXtlsm"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Sample corpus\n",
        "sentences = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Word embeddings capture meaning of words\",\n",
        "    \"Deep learning improves NLP performance\"\n",
        "]\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word vector for a word\n",
        "word_vector = model.wv['nlp']\n",
        "print(word_vector)\n",
        "\n",
        "# Find similar words\n",
        "similar_words = model.wv.most_similar('language')\n",
        "print(similar_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60ccp8e_1eOB"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec, FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taPC_-nM1hKr"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec, FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6Z5epVuqjuo"
      },
      "outputs": [],
      "source": [
        "!pip install scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer # Import both CountVectorizer and TfidfVectorizer\n",
        "import numpy as np # Add this line to import numpy\n",
        "from gensim.models import Word2Vec, FastText # Import Word2Vec and FastText here\n",
        "\n",
        "# Load dataset\n",
        "train_file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "test_file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "\n",
        "df_train = pd.read_csv(train_file_path)\n",
        "df_test = pd.read_csv(test_file_path)\n",
        "\n",
        "# Rename columns (Modify according to your dataset)\n",
        "# Assuming your text column is named 'pichwara se akhrot kahe fod rahe ho chicha' and label column is named '1'\n",
        "df_train.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "df_test.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_train[\"text\"], df_train[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Bag of Words (BoW)\n",
        "vectorizer_bow = CountVectorizer()\n",
        "X_train_bow = vectorizer_bow.fit_transform(X_train)\n",
        "X_test_bow = vectorizer_bow.transform(X_test)\n",
        "\n",
        "# 2. TF-IDF\n",
        "vectorizer_tfidf = TfidfVectorizer() # Now TfidfVectorizer is properly imported and can be used\n",
        "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer_tfidf.transform(X_test)\n",
        "\n",
        "# 3. Word2Vec\n",
        "w2v_model = Word2Vec(sentences=[text.split() for text in X_train], vector_size=100, window=5, min_count=1, workers=4)\n",
        "X_train_w2v = np.array([np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv] or [np.zeros(100)], axis=0) for text in X_train])\n",
        "X_test_w2v = np.array([np.mean([w2v_model.wv[word] for word in text.split() if word in w2v_model.wv] or [np.zeros(100)], axis=0) for text in X_test])\n",
        "\n",
        "# 4. FastText\n",
        "fasttext_model = FastText(sentences=[text.split() for text in X_train], vector_size=100, window=5, min_count=1, workers=4)\n",
        "X_train_fasttext = np.array([np.mean([fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for text in X_train])\n",
        "X_test_fasttext = np.array([np.mean([fasttext_model.wv[word] for word in text.split() if word in fasttext_model.wv] or [np.zeros(100)], axis=0) for text in X_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJYgINHgqnyU"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score # Import necessary metrics here\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"SVM\": SVC(kernel=\"linear\"),\n",
        "    \"Naïve Bayes\": MultinomialNB(),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
        "    \"KNN\": KNeighborsClassifier(n_neighbors=5)\n",
        "}\n",
        "\n",
        "# Evaluate Models\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_tfidf, y_train)  # Using TF-IDF for feature extraction\n",
        "    y_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "    recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "    results.append([name, accuracy, precision, recall, f1])\n",
        "\n",
        "# Print results\n",
        "df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
        "print(df_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_1_uVR_qrHV"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch # Import the torch module here\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Reshape the input to have a sequence length dimension of 1\n",
        "        x = x.unsqueeze(1)  # Add this line\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "input_dim = 100\n",
        "hidden_dim = 128\n",
        "output_dim = 2\n",
        "\n",
        "model_lstm = LSTMModel(input_dim, hidden_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
        "\n",
        "# Convert FastText vectors into PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_fasttext, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_test_tensor = torch.tensor(X_test_fasttext, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_lstm(X_train_tensor)\n",
        "    loss = criterion(outputs, y_train_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    y_pred = model_lstm(X_test_tensor).argmax(dim=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"LSTM Model -> Accuracy: {accuracy}, Precision: {precision}, Recall: {recall}, F1: {f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_0u9KHzk_oX"
      },
      "outputs": [],
      "source": [
        "df_scraped = pd.read_csv('Dataset/Data_With_Profiles.csv')\n",
        "df_public = pd.read_csv('Dataset/Data_With_Profiles.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuodoYuUj162"
      },
      "outputs": [],
      "source": [
        "# Replace df.head(100) with one of the following options:\n",
        "\n",
        "# Option 1: If you want to view the first 100 rows of df_scraped\n",
        "df_scraped.head(100)\n",
        "\n",
        "# Option 2: If you want to view the first 100 rows of df_public\n",
        "df_public.head(100)\n",
        "\n",
        "# Option 3: If you intended to concatenate df_scraped and df_public and assign to df:\n",
        "df = pd.concat([df_scraped, df_public])  # Concatenate and assign to df\n",
        "df.head(70000)  # Then display the first 100 rows of df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNBcdu-nlq_G"
      },
      "outputs": [],
      "source": [
        "df_scraped.head(70000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJA7Ub4vkQwQ"
      },
      "outputs": [],
      "source": [
        "df.head(700000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba6xMWfpkJoz"
      },
      "outputs": [],
      "source": [
        "df.tail(400000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd3QgMquD_RT"
      },
      "outputs": [],
      "source": [
        "#Removing Emojis\n",
        "\n",
        "def removing_emojis(text):\n",
        "    # Emojis pattern\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                    u\"\\U00002702-\\U000027B0\"\n",
        "                    u\"\\U000024C2-\\U0001F251\"\n",
        "                    u\"\\U0001f926-\\U0001f937\"\n",
        "                    u'\\U00010000-\\U0010ffff'\n",
        "                    u\"\\u200d\"\n",
        "                    u\"\\u2640-\\u2642\"\n",
        "                    u\"\\u2600-\\u2B55\"\n",
        "                    u\"\\u23cf\"\n",
        "                    u\"\\u23e9\"\n",
        "                    u\"\\u231a\"\n",
        "                    u\"\\u3030\"\n",
        "                    u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r' ', text)\n",
        "\n",
        "emojiRemoval = lambda x: removing_emojis(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pndAd1ITkWZp"
      },
      "outputs": [],
      "source": [
        "def get_wordcounts(x):\n",
        "  # This function inputs a string\n",
        "  # Returns the length of the string\n",
        "\tlength = len(str(x).split())\n",
        "\treturn length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bAqIlyrpkfVb"
      },
      "outputs": [],
      "source": [
        "def get_charcounts(x):\n",
        "  # This function inputs a string\n",
        "  # Returns the character count in the string\n",
        "\ts = x.split()\n",
        "\tx = ''.join(s)\n",
        "\treturn len(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOMGYA-8kjxz"
      },
      "outputs": [],
      "source": [
        "def get_avg_wordlength(x):\n",
        "  # This function inputs a string\n",
        "  # Returns the Average words lenght\n",
        "  # formulae is total character in a string / total words\n",
        "\tcount = get_charcounts(x)/get_wordcounts(x)\n",
        "\treturn count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRZFEAiakooD"
      },
      "outputs": [],
      "source": [
        "def get_hashtag_counts(x):\n",
        "  # This function input a string\n",
        "  # And returns the total hashtags in the string\n",
        "  # Because our data is mostly focused on tweets\n",
        "  # So for EDA it is important to know the hasttag counts\n",
        "\tl = len([t for t in x.split() if t.startswith('#')])\n",
        "\treturn l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI8BePE8kss6"
      },
      "outputs": [],
      "source": [
        "def get_mentions_counts(x):\n",
        "  # This function inputs a string\n",
        "  # Returns the mention to other users in the tweet\n",
        "\tl = len([t for t in x.split() if t.startswith('@')])\n",
        "\treturn l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0X_7eZ0kxtb"
      },
      "outputs": [],
      "source": [
        "def get_digit_counts(x):\n",
        "  # This function inputs a string\n",
        "  # Return the digits in a string\n",
        "  # important in EDA to know if the tweets contain\n",
        "  # numbers like phone numbers etc.\n",
        "\tdigits = re.findall(r'[0-9,.]+', x)\n",
        "\treturn len(digits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffKNOuEuk3Pb"
      },
      "outputs": [],
      "source": [
        "def get_basic_features(df,col):\n",
        "  df['char_counts'] = df[col].apply(lambda x: get_charcounts(x))\n",
        "  df['word_counts'] = df[col].apply(lambda x: get_wordcounts(x))\n",
        "  df['avg_wordlength'] = df[col].apply(lambda x: get_avg_wordlength(x))\n",
        "  df['hashtag_counts'] = df[col].apply(lambda x: get_hashtag_counts(x))\n",
        "  df['mentions_counts'] = df[col].apply(lambda x: get_mentions_counts(x))\n",
        "  df['digits_counts'] = df[col].apply(lambda x: get_digit_counts(x))\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqubHH4Lk8DD"
      },
      "outputs": [],
      "source": [
        "def remove_emails(x):\n",
        "\treturn re.sub(r'([a-z0-9+._-]+@[a-z0-9+._-]+\\.[a-z0-9+_-]+)',\"\", x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "unuzc_TGlDAh"
      },
      "outputs": [],
      "source": [
        "def remove_urls(x):\n",
        "\treturn re.sub(r'(http|https|ftp|ssh)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', '' , x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZjlrjLtlHPi"
      },
      "outputs": [],
      "source": [
        "def remove_html_tags(x):\n",
        "\treturn BeautifulSoup(x, 'lxml').get_text().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbEylj3qlLN2"
      },
      "outputs": [],
      "source": [
        "def remove_rt(x):\n",
        "\treturn re.sub(r'\\brt\\b', '', x).strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwQZurT8lO2U"
      },
      "outputs": [],
      "source": [
        "def remove_accented_chars(x):\n",
        "\tx = unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idMXOEdGlUCK"
      },
      "outputs": [],
      "source": [
        "def remove_special_chars(x):\n",
        "\tx = re.sub(r'[^\\w ]+', \"\", x)\n",
        "\tx = ' '.join(x.split())\n",
        "\treturn x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cq2fLHRtlX33"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def get_clean(x):\n",
        "    x = str(x).lower().replace('\\'', '').replace('_', ' ')\n",
        "    x = remove_emails(x)\n",
        "    x = remove_urls(x)\n",
        "    x = remove_html_tags(x)\n",
        "    x = remove_rt(x)\n",
        "    x = remove_accented_chars(x)\n",
        "    x = remove_special_chars(x)\n",
        "    x = re.sub(r\"(.)\\1{2,}\", r\"\\1\", x)\n",
        "    return x\n",
        "    x = remove_html_tags(x)\n",
        "    x = remove_rt(x)\n",
        "    x = remove_accented_chars(x)\n",
        "    x = remove_special_chars(x)\n",
        "    x = re.sub(\"(.)\\1{2,}\", \"\\1\", x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY-VLmPsnUkG"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_scraped, df_public])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyiZ2czYnfkS"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pf071vv_l4VI"
      },
      "outputs": [],
      "source": [
        "df.head(700000) # Change df_for_eda to df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yy0JLg3nmbQy"
      },
      "outputs": [],
      "source": [
        "df = pd.concat([df_scraped, df_public])\n",
        "# Check the column names of the DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# Ensure the column you want to access is named 'Label'\n",
        "# If it has a different name, use the correct name\n",
        "# For example, if the column is named 'label', use:\n",
        "# df['label'].value_counts()\n",
        "\n",
        "# Alternatively, if the column is missing, you may need to\n",
        "# review how the DataFrame was created to include the column\n",
        "# or load the data again, ensuring the column is present."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKr7fZ8gmScf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt # Import the matplotlib.pyplot module\n",
        "\n",
        "# To plot using matplotlib\n",
        "# Let us define some parameters\n",
        "plt.rcParams['figure.figsize'] = [8,5]\n",
        "plt.rcParams['figure.dpi'] = 120"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-k-rAHOjAC-h"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dropout, Dense, GlobalMaxPooling1D # Import Embedding\n",
        "\n",
        "vec_size = 100\n",
        "# Define vocab_size and max_length before using them in the Embedding layer\n",
        "vocab_size = 10000  # Replace with the actual vocabulary size of your dataset\n",
        "max_length = 100    # Replace with the maximum length of your sequences\n",
        "\n",
        "# Now you can use Sequential\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,vec_size,input_length=max_length))\n",
        "\n",
        "model.add(Conv1D(32,2,activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(16,activation='relu'))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1,activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VKPJksjpVwX"
      },
      "outputs": [],
      "source": [
        "vec_size = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size,vec_size,input_length=max_length))\n",
        "\n",
        "model.add(Conv1D(32,2,activation='relu'))\n",
        "model.add(MaxPooling1D(2))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(16,activation='relu'))\n",
        "\n",
        "model.add(GlobalMaxPooling1D())\n",
        "\n",
        "model.add(Dense(1,activation=\"sigmoid\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noO-B1pnpavJ"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcWGbnSIEffJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jt9l_WPbRTXu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option(\"display.max_colwidth\", None)  # Change -1 to None or a nonnegative integer\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RREZBo2hNMGm"
      },
      "outputs": [],
      "source": [
        "#url = 'https://raw.githubusercontent.com/haroonshakeel/roman_urdu_hate_speech/refs/heads/main/task_1_train.tsv'\n",
        "#df = pd.read_csv(url, sep='\\t')  # Specify the delimiter as '\\t' for tab-separated values\n",
        "#df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFWFEBvZMbXi"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdU_fBLVN__W"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTqQoaGbOhef"
      },
      "outputs": [],
      "source": [
        "# Check the column names of your DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# If the column is named differently, replace 'tag' with the correct name\n",
        "# For example, if the column is named 'label', use:\n",
        "# df['label'].value_counts(normalize=True).plot(kind='bar', title='Ratio of observations')\n",
        "\n",
        "# If the column is missing, review how you loaded the data and ensure\n",
        "# the 'tag' column is present in the data file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gB2uUiBpPoAn"
      },
      "outputs": [],
      "source": [
        "#Function to tokenize the text and remove punctuations\n",
        "import string\n",
        "def tokenize_remove_punctuations(text):\n",
        "    clean_text = []\n",
        "    text = text.split(\" \")\n",
        "    for word in text:\n",
        "        word = list(word)\n",
        "        new_word = []\n",
        "        for c in word:\n",
        "            if c not in string.punctuation:\n",
        "                new_word.append(c)\n",
        "        word = \"\".join(new_word)\n",
        "        if len(word)>0:\n",
        "            clean_text.append(word)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4W0NVnQPws8"
      },
      "outputs": [],
      "source": [
        "trial_text = \"hello @anyone reading? wt is the name of am in that this  ??!@\"\n",
        "trial_text = tokenize_remove_punctuations(trial_text)\n",
        "print(trial_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYfnU5w3P7st"
      },
      "outputs": [],
      "source": [
        "#Function to remove english stopwords\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "def remove_stopwords(text):\n",
        "    clean_text = []\n",
        "    for word in text:\n",
        "        if word not in stopwords:\n",
        "            clean_text.append(word)\n",
        "    return clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUG1uMF8RNZX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evLkEEUgRfxO"
      },
      "outputs": [],
      "source": [
        "import pickle # Import the pickle module\n",
        "# Import the TfidfVectorizer class\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC  # Import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB # Import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression # Import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier # Import KNeighborsClassifier\n",
        "\n",
        "# ... (Your code to define and fit the vectorizer, e.g., TfidfVectorizer) ...\n",
        "\n",
        "# Assuming 'vectorizer' is defined and fitted here:\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Initialize the models\n",
        "model4 = LinearSVC()  # Initialize the model\n",
        "model3 = MultinomialNB() # Initialize MultinomialNB\n",
        "model2 = LogisticRegression() # Initialize LogisticRegression\n",
        "model1 = KNeighborsClassifier() # Initialize KNeighborsClassifier\n",
        "# ... (Code to fit the model using your training data) ...\n",
        "\n",
        "pickle_out = open(\"vectorizer.pkl\",\"wb\")\n",
        "pickle.dump(vectorizer, pickle_out)  # Changed count_vector to vectorizer\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"LinearSVC.pkl\",\"wb\")\n",
        "pickle.dump(model4, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"MultinomialNB.pkl\",\"wb\")\n",
        "pickle.dump(model3, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"LogisticRegression.pkl\",\"wb\")\n",
        "pickle.dump(model2, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"KNeighborsClassifier.pkl\",\"wb\")\n",
        "pickle.dump(model1, pickle_out)\n",
        "pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR1QldKwRq2W"
      },
      "outputs": [],
      "source": [
        "model_params = {\n",
        "    'LinearSVC': {\n",
        "        'model': LinearSVC(max_iter=1000000,random_state=42),\n",
        "        'params' : {\n",
        "            'C': [0.1,1,5,10,20],\n",
        "        }\n",
        "    },\n",
        "    'MultinomialNB': {\n",
        "        'model': MultinomialNB(),\n",
        "        'params' : {\n",
        "            'alpha': np.linspace(0.5, 1.5, 6),\n",
        "            'fit_prior': [True, False],\n",
        "        }\n",
        "    },\n",
        "    'logistic_regression' : {\n",
        "        'model': LogisticRegression(random_state=42, multi_class='ovr',max_iter=1000000),\n",
        "        'params': {\n",
        "            'C': [1,5,10],\n",
        "            'solver': ['newton-cg', 'lbfgs', 'liblinear'],\n",
        "        }\n",
        "    },\n",
        "    'KNeighborsClassifier': {\n",
        "        'model': KNeighborsClassifier(p=2 ),\n",
        "        'params' : {\n",
        "            'n_neighbors': [5,9,11,23],\n",
        "              'weights' : ['uniform', 'distance'],\n",
        "              'metric' : ['euclidean', 'manhattan', 'minkowski'],\n",
        "        }\n",
        "    }\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pI_LpDVoTU_N"
      },
      "outputs": [],
      "source": [
        "# Rename columns for better clarity\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Now, extract features and target variable\n",
        "X = df[\"text\"]  # Use 'text' as features\n",
        "y = df[\"label\"]  # Use 'label' as target variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AlDVD8aFTYjL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define model parameters (make sure model_params is defined in your script)\n",
        "scores = []\n",
        "for model_name, mp in model_params.items():\n",
        "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "\n",
        "    # Fit the model using the vectorized training data\n",
        "    clf.fit(X_train_vec, y_train)\n",
        "\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "# Store results in a DataFrame\n",
        "params = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "OxMTn3P8Rvzg"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define model parameters (make sure model_params is defined in your script)\n",
        "scores = []\n",
        "for model_name, mp in model_params.items():\n",
        "    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n",
        "\n",
        "    # Fit the model using the vectorized training data\n",
        "    clf.fit(X_train_vec, y_train)  # Changed line to use vectorized data\n",
        "\n",
        "    scores.append({\n",
        "        'model': model_name,\n",
        "        'best_score': clf.best_score_,\n",
        "        'best_params': clf.best_params_\n",
        "    })\n",
        "\n",
        "# Store results in a DataFrame\n",
        "params = pd.DataFrame(scores, columns=['model', 'best_score', 'best_params'])\n",
        "params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdlTNcKyWEiz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the KNeighborsClassifier with the vectorized data\n",
        "model1 = KNeighborsClassifier(n_neighbors=23, metric='euclidean', weights='distance')\n",
        "model1.fit(X_train_vec, y_train)  # Use vectorized training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9aXpNSFWR_D"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the KNeighborsClassifier with the vectorized data\n",
        "model1 = KNeighborsClassifier(n_neighbors=23, metric='euclidean', weights='distance')\n",
        "model1.fit(X_train_vec, y_train)  # Use vectorized training data\n",
        "\n",
        "# Initialize the 'results' list before appending to it\n",
        "results = []  # This line is added\n",
        "\n",
        "# ---->>>> Changed to use X_test_vec and X_train_vec which are TFIDF vector representations of text\n",
        "predictions_test = model1.predict(X_test_vec)\n",
        "predictions_train = model1.predict(X_train_vec)\n",
        "\n",
        "temp = ['KNeighborsClassifier']\n",
        "temp.append(accuracy_score(predictions_train,y_train))\n",
        "temp.append(recall_score(predictions_train,y_train))\n",
        "temp.append(f1_score(predictions_train,y_train))\n",
        "temp.append(precision_score(predictions_train,y_train))\n",
        "temp.append(accuracy_score(predictions_test,y_test))\n",
        "temp.append(recall_score(predictions_test,y_test))\n",
        "temp.append(f1_score(predictions_test,y_test))\n",
        "temp.append(precision_score(predictions_test,y_test))\n",
        "results.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo98uZtEW8QJ"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(predictions_test,y_test)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5G3HroLXBNA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the LogisticRegression with the vectorized data\n",
        "model2 = LogisticRegression(C=10, random_state=42, solver='lbfgs', multi_class='ovr',max_iter=1000000)\n",
        "model2.fit(X_train_vec, y_train) # Use vectorized training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGVnmWdnXFo9"
      },
      "outputs": [],
      "source": [
        "predictions_test = model2.predict(X_test_vec)  # Changed to use X_test_vec\n",
        "predictions_train = model2.predict(X_train_vec)  # Changed to use X_train_vec\n",
        "temp = ['LogisticRegression']\n",
        "temp.append(accuracy_score(predictions_train,y_train))\n",
        "temp.append(recall_score(predictions_train,y_train))\n",
        "temp.append(f1_score(predictions_train,y_train))\n",
        "temp.append(precision_score(predictions_train,y_train))\n",
        "temp.append(accuracy_score(predictions_test,y_test))\n",
        "temp.append(recall_score(predictions_test,y_test))\n",
        "temp.append(f1_score(predictions_test,y_test))\n",
        "temp.append(precision_score(predictions_test,y_test))\n",
        "results.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUSCwoNtXHEk"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(predictions_test,y_test)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWHIFitnXLf4"
      },
      "outputs": [],
      "source": [
        "model3 = MultinomialNB(alpha=0.5, fit_prior=True) # Change 'True' to True\n",
        "model3.fit(X_train_vec, y_train) # Use the vectorized training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHlYzbnjX_yJ"
      },
      "outputs": [],
      "source": [
        "predictions_test = model3.predict(X_test_vec)  # Use the vectorized test data\n",
        "predictions_train = model3.predict(X_train_vec) # Use the vectorized training data\n",
        "temp = ['MultinomialNB']\n",
        "temp.append(accuracy_score(predictions_train,y_train))\n",
        "temp.append(recall_score(predictions_train,y_train))\n",
        "temp.append(f1_score(predictions_train,y_train))\n",
        "temp.append(precision_score(predictions_train,y_train))\n",
        "temp.append(accuracy_score(predictions_test,y_test))\n",
        "temp.append(recall_score(predictions_test,y_test))\n",
        "temp.append(f1_score(predictions_test,y_test))\n",
        "temp.append(precision_score(predictions_test,y_test))\n",
        "results.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ4cackgYKPB"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(predictions_test,y_test)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xSV-wjRYN9a"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression  # Import LogisticRegression\n",
        "from sklearn.svm import LinearSVC # Import LinearSVC\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer to the training data and transform it\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the test data using the fitted vectorizer\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the LinearSVC with the *vectorized* data\n",
        "model4 = LinearSVC(C=1, max_iter=1000000, random_state=42)\n",
        "model4.fit(X_train_vec, y_train)  # Use X_train_vec instead of X_train\n",
        "\n",
        "# ... (rest of your code) ...\n",
        "\n",
        "# When making predictions, use the vectorized test data:\n",
        "predictions_test = model4.predict(X_test_vec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F21pOe-1YTkj"
      },
      "outputs": [],
      "source": [
        "predictions_test = model4.predict(X_test_vec)  # Use X_test_vec for predictions\n",
        "predictions_train = model4.predict(X_train_vec) # Use X_train_vec for predictions\n",
        "temp = ['LinearSVC']\n",
        "temp.append(accuracy_score(predictions_train,y_train))\n",
        "temp.append(recall_score(predictions_train,y_train))\n",
        "temp.append(f1_score(predictions_train,y_train))\n",
        "temp.append(precision_score(predictions_train,y_train))\n",
        "temp.append(accuracy_score(predictions_test,y_test))\n",
        "temp.append(recall_score(predictions_test,y_test))\n",
        "temp.append(f1_score(predictions_test,y_test))\n",
        "temp.append(precision_score(predictions_test,y_test))\n",
        "results.append(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ShzDY_HYYPS"
      },
      "outputs": [],
      "source": [
        "confusion = confusion_matrix(predictions_test,y_test)\n",
        "print(confusion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6QT3UWCYbrR"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(results, columns = ['Algorithm','Accuracy Score : Train', 'Recall Score : Train','F1-Score :Train','Precision Score :Train','Accuracy Score : Test', 'Recall Score : Test','F1-Score : Test','Precision Score : Test'])\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FoyETLOYfK6"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XADrzApJYjel"
      },
      "outputs": [],
      "source": [
        "import pickle # Import the pickle module\n",
        "\n",
        "pickle_out = open(\"vectorizer.pkl\",\"wb\")\n",
        "pickle.dump(vectorizer, pickle_out)  # Changed count_vector to vectorizer\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"LinearSVC.pkl\",\"wb\")\n",
        "pickle.dump(model4, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"MultinomialNB.pkl\",\"wb\")\n",
        "pickle.dump(model3, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"LogisticRegression.pkl\",\"wb\")\n",
        "pickle.dump(model2, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_out = open(\"KNeighborsClassifier.pkl\",\"wb\")\n",
        "pickle.dump(model1, pickle_out)\n",
        "pickle_out.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHlkAts3bcWA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"Dataset/Data_With_Profiles.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\n",
        "    \"pichwara se akhrot kahe fod rahe ho chicha\": \"text\",\n",
        "    \"1\": \"label\"\n",
        "}, inplace=True)\n",
        "\n",
        "# Extract features and target variable\n",
        "X = df[\"text\"]\n",
        "y = df[\"label\"]\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Save the vectorizer\n",
        "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "# Define ML Models\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(max_iter=10000, random_state=42),\n",
        "    \"LinearSVC\": LinearSVC(max_iter=10000, random_state=42),\n",
        "    \"MultinomialNB\": MultinomialNB(),\n",
        "    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=23, metric='euclidean', weights='distance'),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier(random_state=42),\n",
        "    \"RandomForestClassifier\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    \"GradientBoostingClassifier\": GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
        "    \"AdaBoostClassifier\": AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "# Store results\n",
        "results = []\n",
        "\n",
        "# Train each model and evaluate\n",
        "for model_name, model in models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train_vec, y_train)\n",
        "\n",
        "    # Save model\n",
        "    with open(f\"{model_name}.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions_train = model.predict(X_train_vec)\n",
        "    predictions_test = model.predict(X_test_vec)\n",
        "\n",
        "    # Store performance metrics\n",
        "    temp = [model_name]\n",
        "    temp.append(accuracy_score(y_train, predictions_train))\n",
        "    temp.append(recall_score(y_train, predictions_train))\n",
        "    temp.append(f1_score(y_train, predictions_train))\n",
        "    temp.append(precision_score(y_train, predictions_train))\n",
        "    temp.append(accuracy_score(y_test, predictions_test))\n",
        "    temp.append(recall_score(y_test, predictions_test))\n",
        "    temp.append(f1_score(y_test, predictions_test))\n",
        "    temp.append(precision_score(y_test, predictions_test))\n",
        "\n",
        "    results.append(temp)\n",
        "\n",
        "\n",
        "# Convert results to DataFrame\n",
        "columns = [\"Model\", \"Train Accuracy\", \"Train Recall\", \"Train F1\", \"Train Precision\",\n",
        "           \"Test Accuracy\", \"Test Recall\", \"Test F1\", \"Test Precision\"]\n",
        "df_results = pd.DataFrame(results, columns=columns) # Removed extra space before this line\n",
        "\n",
        "# Display results\n",
        "print(df_results)\n",
        "\n",
        "# Save results\n",
        "df_results.to_csv(\"model_performance.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtGmUWFmgaC8"
      },
      "outputs": [],
      "source": [
        "# Convert results to DataFrame\n",
        "columns = [\"Model\", \"Train Accuracy\", \"Train Recall\", \"Train F1\", \"Train Precision\",\n",
        "           \"Test Accuracy\", \"Test Recall\", \"Test F1\", \"Test Precision\"]\n",
        "df_results = pd.DataFrame(results, columns=columns) # Removed extra space before this line\n",
        "\n",
        "# Display results\n",
        "print(df_results)\n",
        "\n",
        "# Save results\n",
        "df_results.to_csv(\"model_performance.csv\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OH8KsYcId9Q2"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame(results, columns = ['Algorithm','Accuracy Score : Train', 'Recall Score : Train','F1-Score :Train','Precision Score :Train','Accuracy Score : Test', 'Recall Score : Test','F1-Score : Test','Precision Score : Test'])\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqzAOxB9F7bJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Dataset/Data_With_Profiles.csv\")\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "\n",
        "# Extract features and labels\n",
        "X, y = df[\"text\"], df[\"label\"]\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TF-IDF Feature Extraction\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Define models with optimized parameters for speed\n",
        "models = {\n",
        "    'LogisticRegression': LogisticRegression(C=1, solver='liblinear', max_iter=1000),\n",
        "    'LinearSVC': LinearSVC(C=1, max_iter=1000),\n",
        "    'MultinomialNB': MultinomialNB(alpha=1.0),\n",
        "    'KNeighborsClassifier': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
        "    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=10, criterion='gini'),\n",
        "    'RandomForestClassifier': RandomForestClassifier(n_estimators=100, max_depth=10),\n",
        "    'GradientBoostingClassifier': GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n",
        "    'AdaBoostClassifier': AdaBoostClassifier(n_estimators=100, learning_rate=0.1)\n",
        "}\n",
        "\n",
        "# Train models and store scores\n",
        "results = []\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train_vec, y_train)\n",
        "    test_acc = model.score(X_test_vec, y_test)\n",
        "    results.append({'Model': name, 'Accuracy': test_acc})\n",
        "\n",
        "# Convert results to a DataFrame\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
        "print(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJsqajg6iWtx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, Bidirectional, Conv1D, GlobalMaxPooling1D, Embedding, Dropout, Flatten, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"Dataset/Data_With_Profiles.csv\")\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "\n",
        "# Convert text to string and extract labels\n",
        "X, y = df[\"text\"].astype(str).values, df[\"label\"].values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenization & Padding\n",
        "max_words, max_len, embedding_dim = 10000, 100, 128\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding=\"post\")\n",
        "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding=\"post\")\n",
        "\n",
        "# Function to build models\n",
        "def build_model(model_type):\n",
        "    model = Sequential([Embedding(max_words, embedding_dim, input_length=max_len)])\n",
        "\n",
        "    if model_type == \"MLP\":\n",
        "        model.add(Flatten())\n",
        "    elif model_type == \"LSTM\":\n",
        "        model.add(LSTM(64))\n",
        "    elif model_type == \"BiLSTM\":\n",
        "        model.add(Bidirectional(LSTM(64)))\n",
        "    elif model_type == \"CNN\":\n",
        "        model.add(Conv1D(64, 5, activation='relu'))\n",
        "        model.add(GlobalMaxPooling1D())\n",
        "    elif model_type == \"CNN_LSTM\":\n",
        "        model.add(Conv1D(64, 5, activation='relu'))\n",
        "        model.add(LSTM(64))\n",
        "\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define models\n",
        "dl_models = {name: build_model(name) for name in [\"MLP\", \"LSTM\", \"BiLSTM\", \"CNN\", \"CNN_LSTM\"]}\n",
        "\n",
        "# Train & Evaluate Models\n",
        "results = []\n",
        "for model_name, model in dl_models.items():\n",
        "    print(f\"Training {model_name}...\")\n",
        "\n",
        "    model.fit(X_train_seq, y_train, epochs=3, batch_size=32, validation_data=(X_test_seq, y_test), verbose=1)\n",
        "\n",
        "    # Predictions\n",
        "    pred_train = (model.predict(X_train_seq) > 0.5).astype(\"int32\")\n",
        "    pred_test = (model.predict(X_test_seq) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # Store performance\n",
        "    results.append([\n",
        "        model_name,\n",
        "        accuracy_score(y_train, pred_train), recall_score(y_train, pred_train), f1_score(y_train, pred_train), precision_score(y_train, pred_train),\n",
        "        accuracy_score(y_test, pred_test), recall_score(y_test, pred_test), f1_score(y_test, pred_test), precision_score(y_test, pred_test)\n",
        "    ])\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df_results = pd.DataFrame(results, columns=[\"Model\", \"Train Accuracy\", \"Train Recall\", \"Train F1\", \"Train Precision\",\n",
        "                                            \"Test Accuracy\", \"Test Recall\", \"Test F1\", \"Test Precision\"])\n",
        "\n",
        "# Display results\n",
        "print(df_results)\n",
        "df_results.to_csv(\"deep_learning_performance.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ftZviSEvrgU"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JmppF8gv28E"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Dataset\n",
        "df_train = pd.read_csv(\"Dataset/Data_With_Profiles.csv\", names=[\"text\", \"label\"], header=0)\n",
        "df_test = pd.read_csv(\"Dataset/Data_With_Profiles.csv\", names=[\"text\", \"label\"], header=0)\n",
        "\n",
        "# Initialize BERT Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Pre-tokenize dataset to save processing time\n",
        "def preprocess_data(texts, labels, tokenizer, max_len=128):\n",
        "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_len)\n",
        "    return CyberbullyingDataset(encodings, labels)\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(self.encodings[\"input_ids\"][idx], dtype=torch.long),\n",
        "            \"attention_mask\": torch.tensor(self.encodings[\"attention_mask\"][idx], dtype=torch.long),\n",
        "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare Tokenized Datasets\n",
        "train_dataset = preprocess_data(df_train[\"text\"], df_train[\"label\"], tokenizer)\n",
        "test_dataset = preprocess_data(df_test[\"text\"], df_test[\"label\"], tokenizer)\n",
        "\n",
        "# Load Pretrained BERT Model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# Training Arguments (Optimized for Speed)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=16,  # Increased batch size for faster training\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=2,  # Reduced epochs for quick evaluation\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate Model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = predictions.predictions.argmax(axis=-1)\n",
        "y_true = df_test[\"label\"].tolist()\n",
        "\n",
        "# Compute Metrics\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "    \"Precision\": precision_score(y_true, y_pred, average=\"weighted\"),\n",
        "    \"Recall\": recall_score(y_true, y_pred, average=\"weighted\"),\n",
        "    \"F1 Score\": f1_score(y_true, y_pred, average=\"weighted\")\n",
        "}\n",
        "\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_kbE1c68yE_"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load Dataset\n",
        "train_file = \"Dataset/Data_With_Profiles.csv\"\n",
        "test_file = \"Dataset/Data_With_Profiles.csv\"\n",
        "\n",
        "df_train = pd.read_csv(train_file)\n",
        "df_test = pd.read_csv(test_file)\n",
        "\n",
        "# Rename columns for consistency\n",
        "df_train.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "df_test.rename(columns={\"pichwara se akhrot kahe fod rahe ho chicha\": \"text\", \"1\": \"label\"}, inplace=True)\n",
        "\n",
        "# Initialize BERT Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "\n",
        "# Custom Dataset Class\n",
        "class CyberbullyingDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.encodings = tokenizer.batch_encode_plus(\n",
        "            texts.tolist(),\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_len,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.labels = torch.tensor(labels.tolist(), dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
        "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
        "            \"labels\": self.labels[idx]\n",
        "        }\n",
        "\n",
        "# Convert DataFrames into Dataset Objects\n",
        "train_dataset = CyberbullyingDataset(df_train[\"text\"], df_train[\"label\"], tokenizer)\n",
        "test_dataset = CyberbullyingDataset(df_test[\"text\"], df_test[\"label\"], tokenizer)\n",
        "\n",
        "# Load Pretrained BERT Model\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=2)\n",
        "\n",
        "# Training Arguments (Optimized for Speed & Memory)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=4,  # Reduce batch size to prevent memory overflow\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset\n",
        ")\n",
        "\n",
        "# Train the Model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the Model\n",
        "predictions = trainer.predict(test_dataset)\n",
        "y_pred = predictions.predictions.argmax(axis=-1)\n",
        "y_true = df_test[\"label\"].tolist()\n",
        "\n",
        "# Compute Metrics\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred, average=\"weighted\")\n",
        "recall = recall_score(y_true, y_pred, average=\"weighted\")\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNNswKa5zO+yu7K9dAN4YJi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPgZeZqmnMVpWb7LrnN3tB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/Google-Drive/blob/main/24_08_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0d851913",
        "outputId": "500d0b12-41ca-4acd-bb43-b3dd6aec7a02"
      },
      "source": [
        "%pip install emoji"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "collapsed": true,
        "id": "aa5898d3",
        "outputId": "c5aa26bc-b529-484c-fee3-2af89f3f425e"
      },
      "source": [
        "%pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
            "Downloading gensim-4.3.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.2/38.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.16.1\n",
            "    Uninstalling scipy-1.16.1:\n",
            "      Successfully uninstalled scipy-1.16.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "c079c0a809df48e7a7187ee4269d0e18"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "39df6ca5",
        "outputId": "84754137-88e8-4992-a57a-918004bf4355"
      },
      "source": [
        "%pip install keras-preprocessing"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-preprocessing\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.12/dist-packages (from keras-preprocessing) (1.26.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from keras-preprocessing) (1.17.0)\n",
            "Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Installing collected packages: keras-preprocessing\n",
            "Successfully installed keras-preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l4jbfjBAThn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Note: The layer has been tested with Keras 2.0.6\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "            # next add a Dense layer (for classification/regression) or whatever...\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight(shape=(input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(shape=(input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        eij = K.squeeze(K.dot(x, K.expand_dims(self.W)), axis=-1)\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
        "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "\n",
        "        weighted_input = x * a\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from keras_preprocessing.text import Tokenizer\n",
        "from nltk import TweetTokenizer\n",
        "from numpy import asarray, zeros\n",
        "\n",
        "\n",
        "def ReadOpen(filename, Labelfile):\n",
        "\tdata = []\n",
        "\ttokenizer_tweet = TweetTokenizer()\n",
        "\n",
        "\twith open(filename, 'r', encoding=\"utf-8\", errors=\"replace\") as readFile:\n",
        "\t\tlines = readFile.readlines()\n",
        "\n",
        "\tfor line in lines:\n",
        "\t\ttemp = []\n",
        "\t\tsentence = ' '.join(line.strip().split(','))\n",
        "\t\tfor token in tokenizer_tweet.tokenize(sentence):\n",
        "\t\t\ttemp.append(token.lower())\n",
        "\t\tdata.append(temp)\n",
        "\n",
        "\tlabels_pd = pd.read_csv(Labelfile, index_col=False, header=None)\n",
        "\tlabels = labels_pd.values.squeeze()\n",
        "\n",
        "\treturn data, labels, len(lines)\n",
        "\n",
        "\n",
        "def AverageVectorPerTweet(data, model_word2vec):\n",
        "    avg = []\n",
        "    for i in range(len(data)):\n",
        "        row = []\n",
        "        for j in data[i]:\n",
        "            if j in model_word2vec.vocab:\n",
        "                row.append(model_word2vec[j])\n",
        "        if row:\n",
        "            row = np.asarray(row)\n",
        "            avg.append((np.average(row, axis=0)).tolist())\n",
        "        else:\n",
        "            avg.append(np.zeros((200,)).tolist())\n",
        "    return avg\n",
        "\n",
        "\n",
        "def AverageVectorPerEmoji(data, model_emoji2vec):\n",
        "    avg = []\n",
        "    for i in range(len(data)):\n",
        "        row = []\n",
        "        for j in data[i]:\n",
        "            if j in model_emoji2vec.vocab:\n",
        "                row.append(model_emoji2vec[j])\n",
        "        if row:\n",
        "            row = np.asarray(row)\n",
        "            avg.append((np.average(row, axis=0)).tolist())\n",
        "        else:\n",
        "            avg.append(np.zeros((200,)).tolist())\n",
        "    return avg\n",
        "\n",
        "\n",
        "def ml_read_data(data_file, label_file, glove_model, emoji2vec_model):\n",
        "    data, label, count = ReadOpen(data_file, label_file)\n",
        "\n",
        "    embedded_sentences = AverageVectorPerTweet(data, glove_model)\n",
        "    embedded_sentences_emoji = AverageVectorPerEmoji(data, emoji2vec_model)\n",
        "    embedded_sentences_emoji = np.concatenate((np.array(embedded_sentences), np.array(embedded_sentences_emoji)),\n",
        "                                              axis=1).tolist()\n",
        "    X = np.array(embedded_sentences)\n",
        "\n",
        "    X = np.array(X.tolist())\n",
        "    y = np.array(label)\n",
        "    indices = np.random.permutation(len(X))\n",
        "\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    X_emoji = np.array(embedded_sentences_emoji)\n",
        "\n",
        "    X_emoji = np.array(X_emoji.tolist())\n",
        "    y_emoji = np.array(label)\n",
        "\n",
        "    X_emoji = X_emoji[indices]\n",
        "    y_emoji = y_emoji[indices]\n",
        "    return X, y, X_emoji, y_emoji\n",
        "\n",
        "\n",
        "def Preprocess(docs, count, glove_model, emoji2vec_model, get_emoji2vec=True):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(docs)\n",
        "    encoded_docs = tokenizer.texts_to_sequences(docs)\n",
        "    padded_docs = pad_sequences(encoded_docs, padding='post')\n",
        "    maxlen = len(padded_docs[0])\n",
        "    nf = 0\n",
        "    embedding_matrix = zeros((count, 200))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if word in glove_model.vocab:\n",
        "            embedding_matrix[i] = glove_model[word]\n",
        "        else:\n",
        "            new_em = []\n",
        "            em = [item['emoji'] for item in emoji.emoji_list(word)]\n",
        "            for ej in em:\n",
        "                for c in ej:\n",
        "                    if emoji.is_emoji(c):\n",
        "                        new_em.append(c)\n",
        "            try:\n",
        "                if new_em:\n",
        "                    row = []\n",
        "                    for e in new_em:\n",
        "                        row.append(emoji2vec_model[e])\n",
        "                    if get_emoji2vec:\n",
        "                        embedding_matrix[i] = np.average(np.asarray(row), axis=0).tolist()\n",
        "                    else:\n",
        "                        embedding_matrix[i] = [0] * 200\n",
        "                else:\n",
        "                    embedding_matrix[i] = [0] * 200\n",
        "            except:\n",
        "                embedding_matrix[i] = [0] * 200\n",
        "                nf += 1\n",
        "\n",
        "    return padded_docs, embedding_matrix, maxlen, tokenizer\n",
        "\n",
        "\n",
        "def preprocess_test(tokenizer, maxlen, test_docs):\n",
        "    test_encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
        "    test_padded_docs = pad_sequences(test_encoded_docs, maxlen=maxlen, padding='post')\n",
        "    return test_padded_docs"
      ],
      "metadata": {
        "id": "WhYeFdMJB_bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dropout, Activation, Bidirectional, Dense, Flatten, Embedding\n",
        "from tensorflow.python.keras.layers.embeddings import Embedding\n",
        "from tensorflow.python.keras.layers.core import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from attention_layer import Attention # Removed this line\n",
        "\n",
        "\n",
        "def PrepModel(count, embedding_matrix, l, lrate=0.001):\n",
        "    model = Sequential()\n",
        "    e = Embedding(count, 200, weights=[embedding_matrix], input_length=l, trainable=False)\n",
        "    model.add(e)\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Bidirectional(\n",
        "        LSTM(256, kernel_initializer='he_normal', recurrent_activation='sigmoid', return_sequences=True,\n",
        "             activation='tanh')))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Bidirectional(\n",
        "        LSTM(256, kernel_initializer='he_normal', recurrent_activation='sigmoid', return_sequences=True,\n",
        "             activation='tanh')))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Attention())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=Adam(lr=lrate), loss='binary_crossentropy', metrics=['acc'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "ChEA6vXwCwZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from gensim.models import KeyedVectors\n",
        "import joblib\n",
        "# from data_utils import ml_read_data # Removed this line"
      ],
      "metadata": {
        "id": "cHh93jAsBM2d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
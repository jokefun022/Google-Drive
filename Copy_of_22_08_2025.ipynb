{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jokefun022/Google-Drive/blob/main/Copy_of_22_08_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c36ac3bf",
        "outputId": "80e7ae90-8bc0-4837-df26-b00548f70073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wMKHF1kEN-H",
        "outputId": "90008c8a-399d-45f3-c6c4-fdd229837b25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== LogisticRegression =====\n",
            "Accuracy: 0.9630\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      1211\n",
            "           1       0.94      0.92      0.93       145\n",
            "           2       0.96      0.94      0.95       628\n",
            "           3       0.86      0.95      0.90        80\n",
            "           4       0.94      0.93      0.94        71\n",
            "\n",
            "    accuracy                           0.96      2135\n",
            "   macro avg       0.94      0.94      0.94      2135\n",
            "weighted avg       0.96      0.96      0.96      2135\n",
            "\n",
            "\n",
            "===== SGDClassifier =====\n",
            "Accuracy: 0.9803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1211\n",
            "           1       0.96      0.94      0.95       145\n",
            "           2       0.98      0.97      0.97       628\n",
            "           3       0.94      0.96      0.95        80\n",
            "           4       0.94      0.96      0.95        71\n",
            "\n",
            "    accuracy                           0.98      2135\n",
            "   macro avg       0.96      0.97      0.96      2135\n",
            "weighted avg       0.98      0.98      0.98      2135\n",
            "\n",
            "\n",
            "===== GaussianNB =====\n",
            "Accuracy: 0.8726\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      1211\n",
            "           1       0.89      0.51      0.65       145\n",
            "           2       0.77      0.92      0.84       628\n",
            "           3       1.00      0.64      0.78        80\n",
            "           4       1.00      0.68      0.81        71\n",
            "\n",
            "    accuracy                           0.87      2135\n",
            "   macro avg       0.92      0.73      0.80      2135\n",
            "weighted avg       0.88      0.87      0.87      2135\n",
            "\n",
            "\n",
            "===== KNeighbors =====\n",
            "Accuracy: 0.9480\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      1211\n",
            "           1       0.80      0.95      0.87       145\n",
            "           2       0.95      0.91      0.93       628\n",
            "           3       0.89      0.95      0.92        80\n",
            "           4       0.89      0.80      0.84        71\n",
            "\n",
            "    accuracy                           0.95      2135\n",
            "   macro avg       0.90      0.92      0.91      2135\n",
            "weighted avg       0.95      0.95      0.95      2135\n",
            "\n",
            "\n",
            "===== RandomForest =====\n",
            "Accuracy: 0.9681\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      1211\n",
            "           1       0.87      0.90      0.88       145\n",
            "           2       0.99      0.95      0.97       628\n",
            "           3       0.94      0.84      0.89        80\n",
            "           4       0.97      0.90      0.93        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.95      0.92      0.93      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "===== AdaBoost =====\n",
            "Accuracy: 0.7911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.95      0.85      1211\n",
            "           1       0.81      0.47      0.59       145\n",
            "           2       0.83      0.60      0.70       628\n",
            "           3       0.99      0.85      0.91        80\n",
            "           4       0.91      0.42      0.58        71\n",
            "\n",
            "    accuracy                           0.79      2135\n",
            "   macro avg       0.86      0.66      0.73      2135\n",
            "weighted avg       0.80      0.79      0.78      2135\n",
            "\n",
            "\n",
            "===== Bagging =====\n",
            "Accuracy: 0.9691\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1211\n",
            "           1       0.88      0.90      0.89       145\n",
            "           2       0.97      0.96      0.97       628\n",
            "           3       0.90      0.82      0.86        80\n",
            "           4       0.86      0.93      0.89        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.92      0.92      0.92      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "===== ExtraTrees =====\n",
            "Accuracy: 0.9766\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1211\n",
            "           1       0.91      0.94      0.92       145\n",
            "           2       0.98      0.97      0.98       628\n",
            "           3       0.97      0.86      0.91        80\n",
            "           4       0.99      0.93      0.96        71\n",
            "\n",
            "    accuracy                           0.98      2135\n",
            "   macro avg       0.97      0.94      0.95      2135\n",
            "weighted avg       0.98      0.98      0.98      2135\n",
            "\n",
            "\n",
            "===== GradientBoosting =====\n",
            "Accuracy: 0.9728\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1211\n",
            "           1       0.96      0.92      0.94       145\n",
            "           2       0.98      0.95      0.96       628\n",
            "           3       0.94      0.97      0.96        80\n",
            "           4       0.91      0.97      0.94        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.95      0.96      0.96      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "==== Summary Accuracies ====\n",
            "SGDClassifier: 0.9803\n",
            "ExtraTrees: 0.9766\n",
            "GradientBoosting: 0.9728\n",
            "Bagging: 0.9691\n",
            "RandomForest: 0.9681\n",
            "LogisticRegression: 0.9630\n",
            "KNeighbors: 0.9480\n",
            "GaussianNB: 0.8726\n",
            "AdaBoost: 0.7911\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "# from Model.BiLSTM_Attention import run_bilstm # This import is not needed as run_bilstm is defined in a previous cell\n",
        "# from Run_ML import run_all_ml # This import might also cause an error if Run_ML is not available\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--model\", choices=[\"bilstm\", \"ml\"], required=False, default=\"ml\") # Set required to False and provide a default\n",
        "    p.add_argument(\"--data_path\", type=str, default=\"/content/Complete Data With Emoji.csv\")\n",
        "    p.add_argument(\"--text_col\", type=str, default=\"Tweet_Text_With_Emoji\")\n",
        "    p.add_argument(\"--label_col\", type=str, default=\"Label\")\n",
        "    # DL params\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--max_len\", type=int, default=80)\n",
        "    p.add_argument(\"--vocab_size\", type=int, default=30000)\n",
        "    p.add_argument(\"--embedding_dim\", type=int, default=128)\n",
        "    p.add_argument(\"--lstm_units\", type=int, default=64)\n",
        "    p.add_argument(\"--lower\", type=lambda s: s.lower() in [\"true\",\"1\",\"yes\"], default=True)\n",
        "    # ML params\n",
        "    p.add_argument(\"--use_emoji\", type=lambda s: s.lower() in [\"true\",\"1\",\"yes\"], default=True)\n",
        "    return p.parse_args([]) # Pass an empty list to avoid argparse issues in Colab\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    if args.model == \"bilstm\":\n",
        "        run_bilstm(args)\n",
        "    else:\n",
        "        run_all_ml(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq-vOSFdFRg1",
        "outputId": "bfac6bee-33e6-4d5e-bd00-467d224704e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== LogisticRegression =====\n",
            "Accuracy: 0.9630\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      1211\n",
            "           1       0.94      0.92      0.93       145\n",
            "           2       0.96      0.94      0.95       628\n",
            "           3       0.86      0.95      0.90        80\n",
            "           4       0.94      0.93      0.94        71\n",
            "\n",
            "    accuracy                           0.96      2135\n",
            "   macro avg       0.94      0.94      0.94      2135\n",
            "weighted avg       0.96      0.96      0.96      2135\n",
            "\n",
            "\n",
            "===== SGDClassifier =====\n",
            "Accuracy: 0.9799\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1211\n",
            "           1       0.96      0.94      0.95       145\n",
            "           2       0.98      0.97      0.97       628\n",
            "           3       0.93      0.96      0.94        80\n",
            "           4       0.94      0.94      0.94        71\n",
            "\n",
            "    accuracy                           0.98      2135\n",
            "   macro avg       0.96      0.96      0.96      2135\n",
            "weighted avg       0.98      0.98      0.98      2135\n",
            "\n",
            "\n",
            "===== GaussianNB =====\n",
            "Accuracy: 0.8726\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      1211\n",
            "           1       0.89      0.51      0.65       145\n",
            "           2       0.77      0.92      0.84       628\n",
            "           3       1.00      0.64      0.78        80\n",
            "           4       1.00      0.68      0.81        71\n",
            "\n",
            "    accuracy                           0.87      2135\n",
            "   macro avg       0.92      0.73      0.80      2135\n",
            "weighted avg       0.88      0.87      0.87      2135\n",
            "\n",
            "\n",
            "===== KNeighbors =====\n",
            "Accuracy: 0.9480\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      1211\n",
            "           1       0.80      0.95      0.87       145\n",
            "           2       0.95      0.91      0.93       628\n",
            "           3       0.89      0.95      0.92        80\n",
            "           4       0.89      0.80      0.84        71\n",
            "\n",
            "    accuracy                           0.95      2135\n",
            "   macro avg       0.90      0.92      0.91      2135\n",
            "weighted avg       0.95      0.95      0.95      2135\n",
            "\n",
            "\n",
            "===== RandomForest =====\n",
            "Accuracy: 0.9681\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      1211\n",
            "           1       0.87      0.90      0.88       145\n",
            "           2       0.99      0.95      0.97       628\n",
            "           3       0.94      0.84      0.89        80\n",
            "           4       0.97      0.90      0.93        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.95      0.92      0.93      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "===== AdaBoost =====\n",
            "Accuracy: 0.7911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.95      0.85      1211\n",
            "           1       0.81      0.47      0.59       145\n",
            "           2       0.83      0.60      0.70       628\n",
            "           3       0.99      0.85      0.91        80\n",
            "           4       0.91      0.42      0.58        71\n",
            "\n",
            "    accuracy                           0.79      2135\n",
            "   macro avg       0.86      0.66      0.73      2135\n",
            "weighted avg       0.80      0.79      0.78      2135\n",
            "\n",
            "\n",
            "===== Bagging =====\n",
            "Accuracy: 0.9691\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1211\n",
            "           1       0.88      0.90      0.89       145\n",
            "           2       0.97      0.96      0.97       628\n",
            "           3       0.90      0.82      0.86        80\n",
            "           4       0.86      0.93      0.89        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.92      0.92      0.92      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "===== ExtraTrees =====\n",
            "Accuracy: 0.9766\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      1211\n",
            "           1       0.91      0.94      0.92       145\n",
            "           2       0.98      0.97      0.98       628\n",
            "           3       0.97      0.86      0.91        80\n",
            "           4       0.99      0.93      0.96        71\n",
            "\n",
            "    accuracy                           0.98      2135\n",
            "   macro avg       0.97      0.94      0.95      2135\n",
            "weighted avg       0.98      0.98      0.98      2135\n",
            "\n",
            "\n",
            "===== GradientBoosting =====\n",
            "Accuracy: 0.9728\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.99      1211\n",
            "           1       0.96      0.92      0.94       145\n",
            "           2       0.98      0.95      0.96       628\n",
            "           3       0.94      0.97      0.96        80\n",
            "           4       0.91      0.97      0.94        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.95      0.96      0.96      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "==== Summary Accuracies ====\n",
            "SGDClassifier: 0.9799\n",
            "ExtraTrees: 0.9766\n",
            "GradientBoosting: 0.9728\n",
            "Bagging: 0.9691\n",
            "RandomForest: 0.9681\n",
            "LogisticRegression: 0.9630\n",
            "KNeighbors: 0.9480\n",
            "GaussianNB: 0.8726\n",
            "AdaBoost: 0.7911\n"
          ]
        }
      ],
      "source": [
        "import argparse, pandas as pd, numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import FeatureUnion, Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def emoji_fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n",
        "\n",
        "def build_emoji_tfidf(train_texts, test_texts):\n",
        "    Xtr_e, Xte_e, _ = emoji_fit_transform(train_texts, test_texts)\n",
        "    return Xtr_e, Xte_e\n",
        "\n",
        "def build_text_features():\n",
        "    word_vec = (\"w\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\", min_df=2))\n",
        "    char_vec = (\"c\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2,5), min_df=2))\n",
        "    return FeatureUnion([word_vec, char_vec])\n",
        "\n",
        "def run_all_ml(args):\n",
        "    df = pd.read_csv(args.data_path, encoding=\"utf-8\")\n",
        "    df = df.dropna(subset=[args.text_col, args.label_col]).reset_index(drop=True)\n",
        "\n",
        "    X_text = df[args.text_col].astype(str)\n",
        "    y_raw = df[args.label_col].astype(str)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "\n",
        "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Text TF-IDF\n",
        "    text_fu = build_text_features()\n",
        "    X_train_text = text_fu.fit_transform(X_train_txt)\n",
        "    X_test_text  = text_fu.transform(X_test_txt)\n",
        "\n",
        "    # Emoji features (optional)\n",
        "    if args.use_emoji:\n",
        "        X_train_emoji, X_test_emoji = build_emoji_tfidf(X_train_txt, X_test_txt)\n",
        "        X_train = hstack([X_train_text, X_train_emoji])\n",
        "        X_test  = hstack([X_test_text,  X_test_emoji])\n",
        "    else:\n",
        "        X_train, X_test = X_train_text, X_test_text\n",
        "\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000, n_jobs=None),\n",
        "        \"SGDClassifier\": SGDClassifier(max_iter=2000, tol=1e-3),\n",
        "        \"GaussianNB\": GaussianNB(),  # requires dense\n",
        "        \"KNeighbors\": KNeighborsClassifier(n_neighbors=5),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "        \"AdaBoost\": AdaBoostClassifier(n_estimators=300, random_state=42),\n",
        "        \"Bagging\": BaggingClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
        "        \"GradientBoosting\": GradientBoostingClassifier(random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n===== {name} =====\")\n",
        "        if name == \"GaussianNB\":\n",
        "            model.fit(X_train.toarray(), y_train)\n",
        "            y_pred = model.predict(X_test.toarray())\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "        results[name] = acc\n",
        "\n",
        "    print(\"\\n==== Summary Accuracies ====\")\n",
        "    for k, v in sorted(results.items(), key=lambda kv: kv[1], reverse=True):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "ap = argparse.ArgumentParser()\n",
        "ap.add_argument(\"--data_path\", type=str, default=\"/content/Complete Data With Emoji.csv\")\n",
        "ap.add_argument(\"--text_col\", type=str, default=\"Tweet_Text_With_Emoji\")\n",
        "ap.add_argument(\"--label_col\", type=str, default=\"Label\")\n",
        "ap.add_argument(\"--use_emoji\", type=lambda s: s.lower() in [\"true\",\"1\",\"yes\"], default=True)\n",
        "args = ap.parse_args([]) # Pass an empty list to avoid argparse issues in Colab\n",
        "run_all_ml(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IZOPMfiyFwYy"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "99NUF6G2FlID"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n",
        "\n",
        "def build_text_union():\n",
        "    w = (\"w\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\", min_df=2))\n",
        "    c = (\"c\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2,5), min_df=2))\n",
        "    return FeatureUnion([w, c])\n",
        "\n",
        "def build_emoji_tfidf(train_texts, test_texts):\n",
        "    Xtr_e, Xte_e, _ = fit_transform(train_texts, test_texts)\n",
        "    return Xtr_e, Xte_e\n",
        "\n",
        "def concat_features(X_text_train, X_text_test, X_emoji_train=None, X_emoji_test=None):\n",
        "    if X_emoji_train is not None and X_emoji_test is not None:\n",
        "        return hstack([X_text_train, X_emoji_train]), hstack([X_text_test, X_emoji_test])\n",
        "    return X_text_train, X_test_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O0TrAYoBEqjC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "class Attention(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, time, feat)\n",
        "        scores = tf.nn.tanh(self.dense(inputs))\n",
        "        weights = tf.nn.softmax(scores, axis=1)\n",
        "        context = tf.reduce_sum(weights * inputs, axis=1)\n",
        "        return context\n",
        "\n",
        "def run_bilstm(args):\n",
        "    df = pd.read_csv(args.data_path, encoding=\"utf-8\")\n",
        "    df = df.dropna(subset=[args.text_col, args.label_col]).reset_index(drop=True)\n",
        "\n",
        "    X = df[args.text_col].astype(str)\n",
        "    if args.lower: X = X.str.lower()\n",
        "    y_raw = df[args.label_col].astype(str)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=args.vocab_size, lower=args.lower, filters=\"\")\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    Xtr = tokenizer.texts_to_sequences(X_train)\n",
        "    Xte = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    Xtr = pad_sequences(Xtr, maxlen=args.max_len, padding=\"post\", truncating=\"post\")\n",
        "    Xte = pad_sequences(Xte, maxlen=args.max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    vocab_size = min(args.vocab_size, len(tokenizer.word_index) + 1)\n",
        "\n",
        "    inp = layers.Input(shape=(args.max_len,))\n",
        "    x = layers.Embedding(vocab_size, args.embedding_dim, mask_zero=True)(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(args.lstm_units, return_sequences=True))(x)\n",
        "    x = Attention()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    if num_classes == 2:\n",
        "        out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        loss = \"binary_crossentropy\"\n",
        "    else:\n",
        "        out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "        loss = \"sparse_categorical_crossentropy\"\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(Xtr, y_train, validation_split=0.1, epochs=args.epochs, batch_size=args.batch_size, verbose=2)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        preds = (model.predict(Xte, verbose=0).ravel() >= 0.5).astype(int)\n",
        "    else:\n",
        "        preds = model.predict(Xte, verbose=0).argmax(axis=1)\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"[BiLSTM+Attention] Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, preds, target_names=le.classes_))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tod_auY4xJSh"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n",
        "\n",
        "def build_text_union():\n",
        "    w = (\"w\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\", min_df=2))\n",
        "    c = (\"c\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2,5), min_df=2))\n",
        "    return FeatureUnion([w, c])\n",
        "\n",
        "def build_emoji_tfidf(train_texts, test_texts):\n",
        "    Xtr_e, Xte_e, _ = fit_transform(train_texts, test_texts)\n",
        "    return Xtr_e, Xte_e\n",
        "\n",
        "def concat_features(X_text_train, X_text_test, X_emoji_train=None, X_emoji_test=None):\n",
        "    if X_emoji_train is not None and X_emoji_test is not None:\n",
        "        return hstack([X_text_train, X_emoji_train]), hstack([X_text_test, X_emoji_test])\n",
        "    return X_text_train, X_text_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VL-krxZZxtit"
      },
      "outputs": [],
      "source": [
        "import re, pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Fm13oiNGxxyD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n",
        "\n",
        "def build_text_union():\n",
        "    w = (\"w\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\", min_df=2))\n",
        "    c = (\"c\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2,5), min_df=2))\n",
        "    return FeatureUnion([w, c])\n",
        "\n",
        "def build_emoji_tfidf(train_texts, test_texts):\n",
        "    Xtr_e, Xte_e, _ = fit_transform(train_texts, test_texts)\n",
        "    return Xtr_e, Xte_e\n",
        "\n",
        "def concat_features(X_text_train, X_text_test, X_emoji_train=None, X_emoji_test=None):\n",
        "    if X_emoji_train is not None and X_emoji_test is not None:\n",
        "        return hstack([X_text_train, X_emoji_train]), hstack([X_text_test, X_emoji_test])\n",
        "    return X_text_train, X_text_test\n",
        "\n",
        "def prepare_features(train_texts, test_texts, use_emoji=True):\n",
        "    text_union = build_text_union()\n",
        "    Xtr_t = text_union.fit_transform(train_texts)\n",
        "    Xte_t = text_union.transform(test_texts)\n",
        "    if use_emoji:\n",
        "        Xtr_e, Xte_e = build_emoji_tfidf(train_texts, test_texts)\n",
        "        return concat_features(Xtr_t, Xte_t, Xtr_e, Xte_e)\n",
        "    return Xtr_t, Xte_t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9VORv0sxx9UQ",
        "outputId": "a9c04984-44d0-49aa-c703-8e5df842edfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def train_word2vec(texts, vector_size=100, window=5, min_count=2):\n",
        "    sentences = [t.split() for t in texts]\n",
        "    model = Word2Vec(sentences=sentences, vector_size=vector_size, window=window, min_count=min_count, workers=4)\n",
        "    return model\n",
        "\n",
        "def build_embedding_matrix(tokenizer: Tokenizer, w2v_model, embedding_dim=100):\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    emb = np.random.normal(scale=0.6, size=(vocab_size, embedding_dim))\n",
        "    for word, idx in tokenizer.word_index.items():\n",
        "        if word in w2v_model.wv:\n",
        "            emb[idx] = w2v_model.wv[word]\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lmyxrQEIye5Z",
        "outputId": "884384b6-c274-4631-f246-1a2621bdb57e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== LogisticRegression =====\n",
            "Accuracy: 0.9630\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      1211\n",
            "           1       0.94      0.92      0.93       145\n",
            "           2       0.96      0.94      0.95       628\n",
            "           3       0.86      0.95      0.90        80\n",
            "           4       0.94      0.93      0.94        71\n",
            "\n",
            "    accuracy                           0.96      2135\n",
            "   macro avg       0.94      0.94      0.94      2135\n",
            "weighted avg       0.96      0.96      0.96      2135\n",
            "\n",
            "\n",
            "===== SGDClassifier =====\n",
            "Accuracy: 0.9803\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99      1211\n",
            "           1       0.96      0.95      0.96       145\n",
            "           2       0.98      0.97      0.97       628\n",
            "           3       0.92      0.97      0.95        80\n",
            "           4       0.96      0.94      0.95        71\n",
            "\n",
            "    accuracy                           0.98      2135\n",
            "   macro avg       0.96      0.97      0.96      2135\n",
            "weighted avg       0.98      0.98      0.98      2135\n",
            "\n",
            "\n",
            "===== GaussianNB =====\n",
            "Accuracy: 0.8726\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.92      0.92      1211\n",
            "           1       0.89      0.51      0.65       145\n",
            "           2       0.77      0.92      0.84       628\n",
            "           3       1.00      0.64      0.78        80\n",
            "           4       1.00      0.68      0.81        71\n",
            "\n",
            "    accuracy                           0.87      2135\n",
            "   macro avg       0.92      0.73      0.80      2135\n",
            "weighted avg       0.88      0.87      0.87      2135\n",
            "\n",
            "\n",
            "===== KNeighbors =====\n",
            "Accuracy: 0.9480\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.98      0.97      1211\n",
            "           1       0.80      0.95      0.87       145\n",
            "           2       0.95      0.91      0.93       628\n",
            "           3       0.89      0.95      0.92        80\n",
            "           4       0.89      0.80      0.84        71\n",
            "\n",
            "    accuracy                           0.95      2135\n",
            "   macro avg       0.90      0.92      0.91      2135\n",
            "weighted avg       0.95      0.95      0.95      2135\n",
            "\n",
            "\n",
            "===== RandomForest =====\n",
            "Accuracy: 0.9681\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99      1211\n",
            "           1       0.87      0.90      0.88       145\n",
            "           2       0.99      0.95      0.97       628\n",
            "           3       0.94      0.84      0.89        80\n",
            "           4       0.97      0.90      0.93        71\n",
            "\n",
            "    accuracy                           0.97      2135\n",
            "   macro avg       0.95      0.92      0.93      2135\n",
            "weighted avg       0.97      0.97      0.97      2135\n",
            "\n",
            "\n",
            "===== AdaBoost =====\n",
            "Accuracy: 0.7911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.95      0.85      1211\n",
            "           1       0.81      0.47      0.59       145\n",
            "           2       0.83      0.60      0.70       628\n",
            "           3       0.99      0.85      0.91        80\n",
            "           4       0.91      0.42      0.58        71\n",
            "\n",
            "    accuracy                           0.79      2135\n",
            "   macro avg       0.86      0.66      0.73      2135\n",
            "weighted avg       0.80      0.79      0.78      2135\n",
            "\n",
            "\n",
            "===== Bagging =====\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
        "from scipy.sparse import hstack\n",
        "import re\n",
        "\n",
        "# Definitions from cell Jq-vOSFdFRg1\n",
        "# Simple emoji-only tokenizer & TF-IDF features\n",
        "EMOJI_REGEX = re.compile(r'[\\U0001F100-\\U0001FAFF\\U00002700-\\U000027BF\\U00002600-\\U000026FF]')\n",
        "\n",
        "def extract_emojis(s: str):\n",
        "    return \" \".join(EMOJI_REGEX.findall(s))\n",
        "\n",
        "def emoji_fit_transform(train_texts, test_texts):\n",
        "    train_emoji = [extract_emojis(t) for t in train_texts]\n",
        "    test_emoji  = [extract_emojis(t) for t in test_texts]\n",
        "    vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(1,3), min_df=1)\n",
        "    Xtr = vec.fit_transform(train_emoji)\n",
        "    Xte = vec.transform(test_emoji)\n",
        "    return Xtr, Xte, vec\n",
        "\n",
        "def build_emoji_tfidf(train_texts, test_texts):\n",
        "    Xtr_e, Xte_e, _ = emoji_fit_transform(train_texts, test_texts)\n",
        "    return Xtr_e, Xte_e\n",
        "\n",
        "def build_text_features():\n",
        "    word_vec = (\"w\", TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), token_pattern=r\"(?u)\\b\\w+\\b|[^\\w\\s]\", min_df=2))\n",
        "    char_vec = (\"c\", TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2,5), min_df=2))\n",
        "    return FeatureUnion([word_vec, char_vec])\n",
        "\n",
        "def run_all_ml(args):\n",
        "    df = pd.read_csv(args.data_path, encoding=\"utf-8\")\n",
        "    df = df.dropna(subset=[args.text_col, args.label_col]).reset_index(drop=True)\n",
        "\n",
        "    X_text = df[args.text_col].astype(str)\n",
        "    y_raw = df[args.label_col].astype(str)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "\n",
        "    X_train_txt, X_test_txt, y_train, y_test = train_test_split(X_text, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    # Text TF-IDF\n",
        "    text_fu = build_text_features()\n",
        "    X_train_text = text_fu.fit_transform(X_train_txt)\n",
        "    X_test_text  = text_fu.transform(X_test_txt)\n",
        "\n",
        "    # Emoji features (optional)\n",
        "    if args.use_emoji:\n",
        "        X_train_emoji, X_test_emoji = build_emoji_tfidf(X_train_txt, X_test_txt)\n",
        "        X_train = hstack([X_train_text, X_train_emoji])\n",
        "        X_test  = hstack([X_test_text,  X_test_emoji])\n",
        "    else:\n",
        "        X_train, X_test = X_train_text, X_test_text\n",
        "\n",
        "    models = {\n",
        "        \"LogisticRegression\": LogisticRegression(max_iter=1000, n_jobs=None),\n",
        "        \"SGDClassifier\": SGDClassifier(max_iter=2000, tol=1e-3),\n",
        "        \"GaussianNB\": GaussianNB(),  # requires dense\n",
        "        \"KNeighbors\": KNeighborsClassifier(n_neighbors=5),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "        \"AdaBoost\": AdaBoostClassifier(n_estimators=300, random_state=42),\n",
        "        \"Bagging\": BaggingClassifier(n_estimators=300, random_state=42, n_jobs=-1),\n",
        "        \"ExtraTrees\": ExtraTreesClassifier(n_estimators=400, random_state=42, n_jobs=-1),\n",
        "        \"GradientBoosting\": GradientBoostingClassifier(random_state=42)\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n===== {name} =====\")\n",
        "        if name == \"GaussianNB\":\n",
        "            model.fit(X_train.toarray(), y_train)\n",
        "            y_pred = model.predict(X_test.toarray())\n",
        "        else:\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        print(f\"Accuracy: {acc:.4f}\")\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "        results[name] = acc\n",
        "\n",
        "    print(\"\\n==== Summary Accuracies ====\")\n",
        "    for k, v in sorted(results.items(), key=lambda kv: kv[1], reverse=True):\n",
        "        print(f\"{k}: {v:.4f}\")\n",
        "\n",
        "# Definitions from cell O0TrAYoBEqjC\n",
        "class Attention(layers.Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = layers.Dense(1)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # inputs: (batch, time, feat)\n",
        "        scores = tf.nn.tanh(self.dense(inputs))\n",
        "        weights = tf.nn.softmax(scores, axis=1)\n",
        "        context = tf.reduce_sum(weights * inputs, axis=1)\n",
        "        return context\n",
        "\n",
        "def run_bilstm(args):\n",
        "    df = pd.read_csv(args.data_path, encoding=\"utf-8\")\n",
        "    df = df.dropna(subset=[args.text_col, args.label_col]).reset_index(drop=True)\n",
        "\n",
        "    X = df[args.text_col].astype(str)\n",
        "    if args.lower: X = X.str.lower()\n",
        "    y_raw = df[args.label_col].astype(str)\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y = le.fit_transform(y_raw)\n",
        "    num_classes = len(le.classes_)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=args.vocab_size, lower=args.lower, filters=\"\")\n",
        "    tokenizer.fit_on_texts(X_train)\n",
        "    Xtr = tokenizer.texts_to_sequences(X_train)\n",
        "    Xte = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "    Xtr = pad_sequences(Xtr, maxlen=args.max_len, padding=\"post\", truncating=\"post\")\n",
        "    Xte = pad_sequences(Xte, maxlen=args.max_len, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "    vocab_size = min(args.vocab_size, len(tokenizer.word_index) + 1)\n",
        "\n",
        "    inp = layers.Input(shape=(args.max_len,))\n",
        "    x = layers.Embedding(vocab_size, args.embedding_dim, mask_zero=True)(inp)\n",
        "    x = layers.Bidirectional(layers.LSTM(args.lstm_units, return_sequences=True))(x)\n",
        "    x = Attention()(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    if num_classes == 2:\n",
        "        out = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "        loss = \"binary_crossentropy\"\n",
        "    else:\n",
        "        out = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "        loss = \"sparse_categorical_crossentropy\"\n",
        "\n",
        "    model = models.Model(inp, out)\n",
        "    model.compile(optimizer=\"adam\", loss=loss, metrics=[\"accuracy\"])\n",
        "    model.summary()\n",
        "\n",
        "    model.fit(Xtr, y_train, validation_split=0.1, epochs=args.epochs, batch_size=args.batch_size, verbose=2)\n",
        "\n",
        "    if num_classes == 2:\n",
        "        preds = (model.predict(Xte, verbose=0).ravel() >= 0.5).astype(int)\n",
        "    else:\n",
        "        preds = model.predict(Xte, verbose=0).argmax(axis=1)\n",
        "\n",
        "    from sklearn.metrics import accuracy_score, classification_report\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    print(f\"[BiLSTM+Attention] Accuracy: {acc:.4f}\")\n",
        "    print(classification_report(y_test, preds, target_names=le.classes_))\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    p = argparse.ArgumentParser()\n",
        "    p.add_argument(\"--model\", choices=[\"bilstm\", \"ml\"], required=False, default=\"ml\") # Set required to False and provide a default\n",
        "    p.add_argument(\"--data_path\", type=str, default=\"/content/Complete Data With Emoji.csv\")\n",
        "    p.add_argument(\"--text_col\", type=str, default=\"Tweet_Text_With_Emoji\")\n",
        "    p.add_argument(\"--label_col\", type=str, default=\"Label\")\n",
        "    # DL params\n",
        "    p.add_argument(\"--epochs\", type=int, default=5)\n",
        "    p.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    p.add_argument(\"--max_len\", type=int, default=80)\n",
        "    p.add_argument(\"--vocab_size\", type=int, default=30000)\n",
        "    p.add_argument(\"--embedding_dim\", type=int, default=128)\n",
        "    p.add_argument(\"--lstm_units\", type=int, default=64)\n",
        "    p.add_argument(\"--lower\", type=lambda s: s.lower() in [\"true\",\"1\",\"yes\"], default=True)\n",
        "    # ML params\n",
        "    p.add_argument(\"--use_emoji\", type=lambda s: s.lower() in [\"true\",\"1\",\"yes\"], default=True)\n",
        "    return p.parse_args([]) # Pass an empty list to avoid argparse issues in Colab\n",
        "\n",
        "args = parse_args()\n",
        "if args.model == \"bilstm\":\n",
        "    run_bilstm(args)\n",
        "else:\n",
        "    run_all_ml(args)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}